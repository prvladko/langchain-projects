{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dd7e386-1321-48b0-8862-d00443f47d0a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d775e93-a233-4b88-9cbe-b79f96206f80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nLarge language models are deep neural networks that are trained on large corpora of text to generate natural language.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(model_name=\"text-davinci-003\")\n",
    "llm(\"explain large language models in one sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c399bdbe-468d-45fd-854a-7c3d75d5c199",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12e312c9-2590-4193-bd9e-e6ef6bbde609",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\",temperature=0.3)\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are an expert data scientist\"),\n",
    "    HumanMessage(content=\"Write a Python script that trains a neural network on simulated data \")\n",
    "]\n",
    "response=chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f9b657f-ab56-4311-99d6-b5bba9abff1a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here is an example Python script that trains a neural network on simulated data:\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "\n",
      "# Generate simulated data\n",
      "X = np.random.rand(1000, 10)\n",
      "y = np.sum(X, axis=1)\n",
      "\n",
      "# Define the neural network architecture\n",
      "model = Sequential()\n",
      "model.add(Dense(64, input_dim=10, activation='relu'))\n",
      "model.add(Dense(32, activation='relu'))\n",
      "model.add(Dense(1, activation='linear'))\n",
      "\n",
      "# Compile the model\n",
      "model.compile(loss='mean_squared_error', optimizer='adam')\n",
      "\n",
      "# Train the model\n",
      "model.fit(X, y, epochs=50, batch_size=32, validation_split=0.2)\n",
      "```\n",
      "\n",
      "In this script, we first generate simulated data using NumPy's `random.rand()` function. We generate 1000 samples with 10 features each, and the target variable is the sum of the features.\n",
      "\n",
      "Next, we define the neural network architecture using Keras' `Sequential` model. We add three fully connected layers with 64, 32, and 1 neurons respectively. The first two layers use the ReLU activation function, while the output layer uses a linear activation function.\n",
      "\n",
      "We then compile the model using the mean squared error loss function and the Adam optimizer.\n",
      "\n",
      "Finally, we train the model on the simulated data using the `fit()` method. We train for 50 epochs with a batch size of 32, and use 20% of the data for validation.\n"
     ]
    }
   ],
   "source": [
    "print(response.content,end='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "006cf1b1-916c-4788-b021-c3e14a5de945",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are an expert data scientist with an expertise on building deep learning models.\n",
    "Explain the concept of {concept} in a couple of lines\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"concept\"],\n",
    "    template=template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc62ef7b-f647-44b3-a703-e5233f6f3b0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['concept'], output_parser=None, partial_variables={}, template='\\nYou are an expert data scientist with an expertise on building deep learning models.\\nExplain the concept of {concept} in a couple of lines\\n', template_format='f-string', validate_template=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edb8d152-e8ac-4b1c-8f4f-24a83474a155",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nRegularization is a technique used in machine learning to prevent overfitting of models, by adding a penalty term to the loss function. This penalty term, which is a hyperparameter, controls the complexity of the model by balancing the bias-variance trade-off. Regularization helps to keep the weights of the model small, and prevents the model from learning the noise of the data.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(prompt.format(concept=\"regularization\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f45331f3-2fe5-4bf6-b3e1-138d0befbff8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAutoencoders are a type of neural network architecture used for unsupervised learning. They are composed of an encoder and a decoder, which aim to reproduce the input data with a reduced number of features, known as the latent representation, which can be used for classification or other tasks.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(prompt.format(concept=\"autoencoder\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f7ced3b-068b-4ef3-b9fa-8cd10bcdc7b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Autoencoders are a type of neural network that learn to compress data using a series of layers. The output of the network is a representation of the input data that is much smaller than the original. Autoencoders are used for compression, denoising, and feature extraction.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Run the chain only specifying the input variable.\n",
    "print(chain.run(\"autoencoder\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a5448ec-128a-494f-9ea7-b6d4422cf266",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "second_prompt = PromptTemplate(\n",
    "    input_variables=[\"ml_concept\"],\n",
    "    template=\"Turn the concept description of {ml_concept} and explain it ti me like I'm five\",\n",
    ")\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e06a88da-0b8f-43b8-adc3-9bfabe9512f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "An autoencoder is a type of artificial neural network that is used to learn an efficient representation, called an encoding, of a set of data (e.g. an image) by training the network to reproduce its input at its output. This is done by learning a compressed representation of the input data and then using it to reconstruct the original input data.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "\n",
      "An autoencoder is like a machine that can take a picture and figure out what shapes and colors are in it. It does this by breaking the picture down into smaller pieces, and then putting it back together again. It remembers which pieces it used and how it put them back together so that it can do the same thing again.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "An autoencoder is like a machine that can take a picture and figure out what shapes and colors are in it. It does this by breaking the picture down into smaller pieces, and then putting it back together again. It remembers which pieces it used and how it put them back together so that it can do the same thing again.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "overall_chain = SimpleSequentialChain(chains=[chain, chain_two], verbose=True)\n",
    "\n",
    "# Run the chain specifying only the input variable for the first chain.\n",
    "explanation = overall_chain.run(\"autoencoder\")\n",
    "print(explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297ded5e-159b-4ae3-bbcb-6600f5f90bcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6ee39f-4652-4c0f-a775-633751d43cde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
